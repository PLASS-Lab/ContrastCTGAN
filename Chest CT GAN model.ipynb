{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity as ssim\n",
    "import lpips\n",
    "from scipy.stats import wasserstein_distance\n",
    "from skimage.color import rgb2lab\n",
    "from skimage.filters import gabor\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=20, scale=(0.90, 1.10)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def visualize_patch(patch, title):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(patch, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def normalize_patch(difference_patch):\n",
    "    min_val = np.min(difference_patch)\n",
    "    max_val = np.max(difference_patch)\n",
    "    \n",
    "    # Avoid division by zero by adding a small constant (e.g., 1e-5) or\n",
    "    # handle the case where max and min values are equal\n",
    "    if max_val - min_val > 1e-5:\n",
    "        normalized_diff_patch = (difference_patch - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_diff_patch = np.zeros(difference_patch.shape, dtype=np.float32)\n",
    "    return normalized_diff_patch\n",
    "\n",
    "\n",
    "class CTDataSet(Dataset):\n",
    "    def __init__(self, root_dir, patient_ids, patch_size=512, sampling_ratio=1, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.patient_ids = patient_ids\n",
    "        self.patch_size = patch_size\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.image_pairs = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for patient_id in patient_ids:\n",
    "            pre_path = os.path.join(root_dir, patient_id, \"POST VUE\")\n",
    "            post_path = os.path.join(root_dir, patient_id, \"POST STD\")\n",
    "\n",
    "            for filename in sorted(os.listdir(pre_path)):\n",
    "                if filename.endswith('.jpg') and '0001.jpg' <= filename <= '0500.jpg':\n",
    "                    pre_image_path = os.path.join(pre_path, filename)\n",
    "                    post_image_path = os.path.join(post_path, filename)\n",
    "                    self.image_pairs.append((pre_image_path, post_image_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        total_patches_per_image_pair = ((512 // self.patch_size) ** 2) * self.sampling_ratio\n",
    "        return int(len(self.image_pairs) * total_patches_per_image_pair)\n",
    "\n",
    "    def load_jpg_image_patch(self, image_path, start_x, start_y):\n",
    "        image = Image.open(image_path)\n",
    "        patch = image.crop((start_x, start_y, start_x + self.patch_size, start_y + self.patch_size))\n",
    "        patch = np.array(patch, dtype=np.float32)\n",
    "\n",
    "        min_val = np.min(patch)\n",
    "        max_val = np.max(patch)\n",
    "        if max_val - min_val > 0:\n",
    "            patch = (patch - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            patch = patch - min_val\n",
    "\n",
    "        return patch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_pair_idx = idx % len(self.image_pairs)\n",
    "        pre_image_path, post_image_path = self.image_pairs[image_pair_idx]\n",
    "\n",
    "        max_x = 512 - self.patch_size\n",
    "        max_y = 512 - self.patch_size\n",
    "        x = random.randint(0, max_x)\n",
    "        y = random.randint(0, max_y)\n",
    "\n",
    "        pre_image_patch = self.load_jpg_image_patch(pre_image_path, x, y)\n",
    "        post_image_patch = self.load_jpg_image_patch(post_image_path, x, y)\n",
    "        \n",
    "        # Create the binary contrast mask\n",
    "        contrast_mask = (post_image_patch >= 0.9).astype(np.float32)\n",
    "\n",
    "        # Convert patches to tensors and add channel dimension\n",
    "        pre_image_patch_tensor = torch.from_numpy(pre_image_patch).unsqueeze(0)\n",
    "        post_image_patch_tensor = torch.from_numpy(post_image_patch).unsqueeze(0)\n",
    "        contrast_mask = (post_image_patch >= 0.9).astype(np.float32)\n",
    "\n",
    "        # Apply the same transformation to all patches\n",
    "        if self.transform:\n",
    "            seed = np.random.randint(2147483647)  # Get a random seed so that we can reproducibly do the transforms.\n",
    "            torch.manual_seed(seed)  # Apply this seed to img transforms\n",
    "            pre_image_patch = self.transform(Image.fromarray(pre_image_patch))\n",
    "            \n",
    "            torch.manual_seed(seed)  # Apply this seed to img transforms\n",
    "            post_image_patch = self.transform(Image.fromarray(post_image_patch))\n",
    "\n",
    "            torch.manual_seed(seed)  # Apply this seed to mask transforms\n",
    "            contrast_mask = self.transform(Image.fromarray(contrast_mask))\n",
    "\n",
    "        return pre_image_patch, post_image_patch, contrast_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DGenerator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet2DGenerator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.enc5 = self.conv_block(512, 512)\n",
    "        self.enc6 = self.conv_block(512, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = self.conv_block(512 + 512, 512)  # Skip connection 추가\n",
    "        self.dec2 = self.conv_block(512 + 512, 512)  # Skip connection 추가\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)  # Skip connection 추가\n",
    "        self.dec4 = self.conv_block(256 + 128, 128)  # Skip connection 추가\n",
    "        self.dec5 = self.conv_block(128 + 64, 64)    # Skip connection 추가\n",
    "\n",
    "        # Final Output\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
    "        e5 = self.enc5(F.max_pool2d(e4, 2))\n",
    "        e6 = self.enc6(F.max_pool2d(e5, 2))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.dec1(torch.cat([F.interpolate(e6, scale_factor=2, mode='nearest'), e5], 1))\n",
    "        d2 = self.dec2(torch.cat([F.interpolate(d1, scale_factor=2, mode='nearest'), e4], 1))\n",
    "        d3 = self.dec3(torch.cat([F.interpolate(d2, scale_factor=2, mode='nearest'), e3], 1))\n",
    "        d4 = self.dec4(torch.cat([F.interpolate(d3, scale_factor=2, mode='nearest'), e2], 1))\n",
    "        d5 = self.dec5(torch.cat([F.interpolate(d4, scale_factor=2, mode='nearest'), e1], 1))\n",
    "\n",
    "        # Final output\n",
    "        out = self.out(d5)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class Discriminator2D(nn.Module):\n",
    "    def __init__(self, in_channels=2):\n",
    "        super(Discriminator2D, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            self.discriminator_block(in_channels, 64, normalization=False),\n",
    "            self.discriminator_block(64, 128),\n",
    "            self.discriminator_block(128, 256),\n",
    "            self.discriminator_block(256, 512),\n",
    "            # 마지막 컨볼루션 레이어의 커널 크기를 1x1로 조정\n",
    "            nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.AdaptiveAvgPool2d(1),  # 모든 입력을 (1, 1) 크기로 평균 풀링\n",
    "\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def discriminator_block(self, in_channels, out_channels, normalization=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "        if normalization:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "        return x.view(-1, 1)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(generated, target, mask, lambda_contrast=10):\n",
    "    l1_loss = nn.L1Loss(reduction='none')\n",
    "    # Apply mask with higher weight to contrast areas\n",
    "    loss = l1_loss(generated, target) * (1 + mask * lambda_contrast)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def load_model_weights(model, weight_path, primary_device):\n",
    "    if os.path.exists(weight_path):\n",
    "        # Load the saved weights\n",
    "        state_dict = torch.load(weight_path, map_location=primary_device)\n",
    "        \n",
    "        # Adjust for DataParallel prefix\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            # No adjustment needed for DataParallel model\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            # Remove 'module.' prefix for single GPU model\n",
    "            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(new_state_dict)\n",
    "        \n",
    "        print(f\"Loaded weights from {weight_path}\")\n",
    "    else:\n",
    "        print(f\"No weights found at {weight_path}, starting from scratch.\")\n",
    "\n",
    "\n",
    "\n",
    "def visualize_images(dataset, G, num_images=10):\n",
    "    plt.figure(figsize=(20, num_images * 5))  \n",
    "\n",
    "    for i, idx in enumerate(random.sample(range(len(dataset)), num_images), start=1):\n",
    "        pre_img, post_img, contrast_mask = dataset[idx]\n",
    "\n",
    "        \n",
    "        if isinstance(pre_img, np.ndarray):\n",
    "            pre_img = torch.from_numpy(pre_img).unsqueeze(0) \n",
    "        if isinstance(post_img, np.ndarray):\n",
    "            post_img = torch.from_numpy(post_img).unsqueeze(0)\n",
    "        if isinstance(contrast_mask, np.ndarray):\n",
    "            contrast_mask = torch.from_numpy(contrast_mask).unsqueeze(0)\n",
    "\n",
    "        pre_img = pre_img.float().unsqueeze(0)  \n",
    "        pre_img = pre_img.to('cuda')  \n",
    "\n",
    "        with torch.no_grad():  \n",
    "            generated_img = G(pre_img).squeeze(0).cpu() \n",
    "\n",
    "        # Displaying the PRE CT Patch\n",
    "        plt.subplot(num_images, 4, i * 4 - 3)\n",
    "        plt.imshow(pre_img.squeeze().cpu().numpy(), cmap='gray')  \n",
    "        plt.title(f'PRE CT Patch {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Displaying the GENERATED POST CT Patch\n",
    "        plt.subplot(num_images, 4, i * 4 - 2)\n",
    "        plt.imshow(generated_img.squeeze().numpy(), cmap='gray')\n",
    "        plt.title(f'Generated POST CT Patch {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Displaying the REAL POST CT Patch\n",
    "        plt.subplot(num_images, 4, i * 4 - 1)\n",
    "        plt.imshow(post_img.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Real POST CT Patch {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Displaying the CONTRAST MASK\n",
    "        plt.subplot(num_images, 4, i * 4)\n",
    "        plt.imshow(contrast_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Contrast Mask {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Dataset splitting\n",
    "root_path = \"/workspace/kyt3426/project_chest_CT_GAN/pilot_data_jpg_dualCT\"\n",
    "all_patient_ids = [name for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "\n",
    "\n",
    "# 훈련 세트 할당\n",
    "train_patient_ids = [\n",
    "    'KP-0003', 'KP-0004', 'KP-0007', 'KP-0010', 'KP-0012', 'KP-0013', 'KP-0014', 'KP-0015', 'KP-0016', \n",
    "    'KP-0018', 'KP-0019', 'KP-0020', 'KP-0021', 'KP-0023', 'KP-0024', 'KP-0026', 'KP-0027', 'KP-0030', \n",
    "    'KP-0031', 'KP-0032', 'KP-0033', 'KP-0034', 'KP-0035', 'KP-0036', 'KP-0037', 'KP-0038', 'KP-0039', \n",
    "    'KP-0040', 'KP-0041', 'KP-0042', 'KP-0043', 'KP-0046', 'KP-0047', 'KP-0048', 'KP-0049', 'KP-0050', \n",
    "    'KP-0051', 'KP-0052', 'KP-0053', 'KP-0055'\n",
    "]\n",
    "\n",
    "# 테스트 세트 할당\n",
    "test_patient_ids = ['KP-0056']\n",
    "\n",
    "train_dataset = CTDataSet(root_path, train_patient_ids, transform=transform_pipeline)\n",
    "test_dataset = CTDataSet(root_path, test_patient_ids, transform=None)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "primary_device = \"cuda:0\"\n",
    "\n",
    "\n",
    "G = UNet2DGenerator(in_channels=1, out_channels=1).to(primary_device)\n",
    "D = Discriminator2D(in_channels=2).to(primary_device)  # Input channels should be 2 since we're concatenating source and target.\n",
    " \n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "device_list = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']\n",
    "if torch.cuda.device_count() > 1:\n",
    "    G = nn.DataParallel(G, device_ids=device_list)\n",
    "    D = nn.DataParallel(D, device_ids=device_list)\n",
    "\n",
    "\n",
    "G.to(primary_device)\n",
    "D.to(primary_device)\n",
    "\n",
    "# # Load pre-trained weights if available\n",
    "load_model_weights(G, 'patch_complicated_lc1_onlynormal_512x512_customloss_N20_Dual_CT_attention_G_epoch_200.pth', primary_device)\n",
    "load_model_weights(D, 'patch_complicated_lc1_onlynormal_512x512_customloss_N20_Dual_CT_attention_D_epoch_200.pth', primary_device)\n",
    "\n",
    "\n",
    "num_epochs = 200000\n",
    "lambda_l1 = 100\n",
    "\n",
    "\n",
    "start_epoch = 0  # Start from the next epoch after the loaded one\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    G.train()\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    \n",
    "    for i, (real_pre_images, real_post_images, contrast_masks) in progress_bar:\n",
    "    \n",
    "        real_pre_images, real_post_images = real_pre_images.to(primary_device), real_post_images.to(primary_device)\n",
    "        contrast_masks = contrast_masks.to(primary_device)  # Make sure masks are on the correct device\n",
    "\n",
    "        current_batch_size = real_pre_images.size(0)\n",
    "\n",
    "        # Discriminator\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        real_labels = torch.ones(current_batch_size, 1).to(primary_device)\n",
    "        real_combined = torch.cat([real_pre_images, real_post_images], dim=1) \n",
    "        outputs = D(real_combined)\n",
    "        loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        # Fake images\n",
    "        fake_post_images = G(real_pre_images)\n",
    "        fake_combined = torch.cat([real_pre_images, fake_post_images.detach()], dim=1)\n",
    "        outputs = D(fake_combined)\n",
    "        fake_labels = torch.zeros(current_batch_size, 1).to(primary_device)\n",
    "        loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Generator \n",
    "        # Generate fake images\n",
    "        fake_post_images = G(real_pre_images)\n",
    "\n",
    "        # Combine with real pre-images for the discriminator\n",
    "        fake_combined = torch.cat([real_pre_images, fake_post_images], dim=1)\n",
    "        outputs = D(fake_combined)\n",
    "\n",
    "        # Calculate GAN loss for the generator\n",
    "        loss_g_gan = criterion(outputs, torch.ones(current_batch_size, 1).to(primary_device))\n",
    "\n",
    "        # Calculate custom L1 loss with contrast mask\n",
    "        loss_g_custom = custom_loss(fake_post_images, real_post_images, contrast_masks, lambda_contrast=1)\n",
    "        \n",
    "        # Combined generator loss\n",
    "        loss_g = loss_g_gan + lambda_l1 * loss_g_custom\n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        progress_bar.set_postfix(Loss_D=loss_d.item(), Loss_G=loss_g.item())\n",
    "\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            visualize_images(test_dataset, G, num_images=10)\n",
    "            \n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        torch.save(G.state_dict(), f'patch_complicated_lc1_onlynormal_512x512_customloss_N20_Dual_CT_attention_G_epoch_{epoch+1}.pth')\n",
    "        torch.save(D.state_dict(), f'patch_complicated_lc1_onlynormal_512x512_customloss_N20_Dual_CT_attention_D_epoch_{epoch+1}.pth')\n",
    "        print(f'Model weights saved for epoch {epoch+1}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "G = UNet2DGenerator(in_channels=1, out_channels=1)\n",
    "\n",
    "# Load the perceptual similarity metric model for LPIPS\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# Weight file path\n",
    "weight_path = '/workspace/kyt3426/project_chest_CT_GAN/patch_complicated_lc0.5_onlynormal_512x512_customloss_N20_Dual_CT_attention_G_epoch_300.pth'\n",
    "\n",
    "# Function to load model weights\n",
    "def load_model_weights(model, weight_path):\n",
    "    state_dict = torch.load(weight_path, map_location='cpu')  # Load to CPU\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Load weights into the model\n",
    "load_model_weights(G, weight_path)\n",
    "\n",
    "# Ensure the model is on the GPU if available\n",
    "G.to(device)\n",
    "\n",
    "# EMD Calculation\n",
    "def calculate_emd(img1, img2):\n",
    "    img1_flat = img1.flatten()\n",
    "    img2_flat = img2.flatten()\n",
    "    emd_value = wasserstein_distance(img1_flat, img2_flat)\n",
    "    return emd_value\n",
    "\n",
    "# Color-Based Similarity Calculation\n",
    "def calculate_color_similarity(img1, img2):\n",
    "    if len(img1.shape) == 2 and len(img2.shape) == 2:\n",
    "        return 0.0\n",
    "    \n",
    "    img1_lab = rgb2lab(img1)\n",
    "    img2_lab = rgb2lab(img2)\n",
    "    \n",
    "    color_dist = distance.euclidean(img1_lab.flatten(), img2_lab.flatten())\n",
    "    return color_dist\n",
    "\n",
    "# Texture-Based Similarity Calculation using Gabor Filters\n",
    "def calculate_texture_similarity(img1, img2):\n",
    "    filt_real1, _ = gabor(img1, frequency=0.6)\n",
    "    filt_real2, _ = gabor(img2, frequency=0.6)\n",
    "    texture_dist = np.linalg.norm(filt_real1 - filt_real2)\n",
    "    return texture_dist\n",
    "\n",
    "# Cosine Similarity Calculation\n",
    "def calculate_cosine_similarity(img1, img2):\n",
    "    img1_flat = img1.flatten().reshape(1, -1)\n",
    "    img2_flat = img2.flatten().reshape(1, -1)\n",
    "    cosine_sim = cosine_similarity(img1_flat, img2_flat)[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "# Euclidean Distance Calculation\n",
    "def calculate_euclidean_distance(img1, img2):\n",
    "    img1_flat = img1.flatten()\n",
    "    img2_flat = img2.flatten()\n",
    "    # Euclidean Distance 계산\n",
    "    euclidean_dist = np.linalg.norm(img1_flat - img2_flat)\n",
    "    return euclidean_dist\n",
    "\n",
    "# Function to calculate all metrics, including cosine similarity and euclidean distance\n",
    "def calculate_metrics(img1, img2):\n",
    "    mae = np.mean(np.abs(img1 - img2))  # MAE\n",
    "    psnr = peak_signal_noise_ratio(img1, img2)  # PSNR\n",
    "    ms_ssim = ssim(img1, img2, data_range=1.0, multichannel=False)  # MS-SSIM\n",
    "    \n",
    "    # LPIPS\n",
    "    img1_tensor = torch.from_numpy(img1).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    img2_tensor = torch.from_numpy(img2).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    lpips_value = lpips_model(img1_tensor, img2_tensor).item()\n",
    "    \n",
    "    emd_value = calculate_emd(img1, img2)  # EMD\n",
    "    color_similarity = calculate_color_similarity(img1, img2)  # Color Similarity\n",
    "    texture_similarity = calculate_texture_similarity(img1, img2)  # Texture Similarity\n",
    "    cosine_sim = calculate_cosine_similarity(img1, img2)  # Cosine Similarity\n",
    "    euclidean_dist = calculate_euclidean_distance(img1, img2)  # Euclidean Distance\n",
    "\n",
    "    return mae, psnr, ms_ssim, lpips_value, emd_value, color_similarity, texture_similarity, cosine_sim, euclidean_dist\n",
    "\n",
    "def inference_and_evaluate(model, input_folder, output_folder, patch_size=512, stride=1):\n",
    "    model.eval()  \n",
    "    results = []\n",
    "    \n",
    "    for patient_id in os.listdir(input_folder):\n",
    "        patient_path = os.path.join(input_folder, patient_id)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "\n",
    "        post_vue_path = os.path.join(patient_path, 'POST VUE')  \n",
    "        post_std_path = os.path.join(patient_path, 'POST STD')  \n",
    "        \n",
    "        if not os.path.exists(post_vue_path) or not os.path.exists(post_std_path):\n",
    "            continue\n",
    "\n",
    "        for image_file in os.listdir(post_vue_path):\n",
    "            if not image_file.endswith('.jpg'):\n",
    "                continue\n",
    "\n",
    "            # Load non-contrast (POST VUE) image\n",
    "            non_contrast_img_path = os.path.join(post_vue_path, image_file)\n",
    "            non_contrast_img = np.array(Image.open(non_contrast_img_path).convert('L'), dtype=np.float32)\n",
    "            non_contrast_img = (non_contrast_img - np.min(non_contrast_img)) / (np.max(non_contrast_img) - np.min(non_contrast_img))  # Normalize\n",
    "\n",
    "            # Load ground truth contrast (POST STD) image\n",
    "            contrast_img_path = os.path.join(post_std_path, image_file)\n",
    "            contrast_img = np.array(Image.open(contrast_img_path).convert('L'), dtype=np.float32)\n",
    "            contrast_img = (contrast_img - np.min(contrast_img)) / (np.max(contrast_img) - np.min(contrast_img))  # Normalize\n",
    "\n",
    "            # Inference: Generate contrast CT from non-contrast CT (using GAN)\n",
    "            image_tensor = torch.from_numpy(non_contrast_img).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                generated_img_tensor = model(image_tensor)\n",
    "            generated_img = generated_img_tensor.squeeze().cpu().numpy()\n",
    "            generated_img = (generated_img - np.min(generated_img)) / (np.max(generated_img) - np.min(generated_img))  # Normalize\n",
    "\n",
    "            # Calculate metrics for (1) GAN-generated vs Ground Truth\n",
    "            mae_gen, psnr_gen, ms_ssim_gen, lpips_gen, emd_gen, color_sim_gen, texture_sim_gen, cosine_sim_gen, euclidean_dist_gen = calculate_metrics(generated_img, contrast_img)\n",
    "            \n",
    "            # Calculate metrics for (2) Non-contrast vs Ground Truth\n",
    "            mae_nc, psnr_nc, ms_ssim_nc, lpips_nc, emd_nc, color_sim_nc, texture_sim_nc, cosine_sim_nc, euclidean_dist_nc = calculate_metrics(non_contrast_img, contrast_img)\n",
    "\n",
    "            # Append results for CSV\n",
    "            results.append({\n",
    "                'Patient ID': patient_id,\n",
    "                'Slice': image_file,\n",
    "                'MAE_GAN_vs_GT': mae_gen,\n",
    "                'PSNR_GAN_vs_GT': psnr_gen,\n",
    "                'MS-SSIM_GAN_vs_GT': ms_ssim_gen,\n",
    "                'LPIPS_GAN_vs_GT': lpips_gen,\n",
    "                'EMD_GAN_vs_GT': emd_gen,\n",
    "                'Color_Similarity_GAN_vs_GT': color_sim_gen,\n",
    "                'Texture_Similarity_GAN_vs_GT': texture_sim_gen,\n",
    "                'Cosine_Similarity_GAN_vs_GT': cosine_sim_gen,\n",
    "                'Euclidean_Distance_GAN_vs_GT': euclidean_dist_gen,\n",
    "                'MAE_NC_vs_GT': mae_nc,\n",
    "                'PSNR_NC_vs_GT': psnr_nc,\n",
    "                'MS-SSIM_NC_vs_GT': ms_ssim_nc,\n",
    "                'LPIPS_NC_vs_GT': lpips_nc,\n",
    "                'EMD_NC_vs_GT': emd_nc,\n",
    "                'Color_Similarity_NC_vs_GT': color_sim_nc,\n",
    "                'Texture_Similarity_NC_vs_GT': texture_sim_nc,\n",
    "                'Cosine_Similarity_NC_vs_GT': cosine_sim_nc,\n",
    "                'Euclidean_Distance_NC_vs_GT': euclidean_dist_nc\n",
    "            })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    output_csv_path = os.path.join(output_folder, 'evaluation_results_updated.csv')\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
